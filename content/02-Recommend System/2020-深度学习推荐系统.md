---
title: "2020-深度推荐系统"
layout: page
date: 2020-06-14
---

## 第一章 

- 推荐系统的意义：帮助用户在"信息过载"的情况下高效地过去兴趣信息；帮助公司做好用户增长，提高用户LTV，从而提升整体价值。
- 推荐系统的架构主要分为数据架构和模型架构。
    - 数据架构部分负责信息的收集与处理，包括离线和实时框架，该部分主要负责样本、特征和数据监控等。
    - 模型架构部分是推荐系统的主体，负责推荐物料的召回与排序，并提供离线评估与在线AB测试等评估模块。

## 第二章 - 深度学习之前的推荐系统

- 协同过滤（Collaborative Filtering）
    - 将"共现矩阵中的向量"通过"相似度计算方法"计算得到相似度，并利用相似度加权将评分求和得到目标值。
    - 基于用户的cf/基于item的cf，分别对应基于用户相似度/基于物品的相似度进行推荐。
- 矩阵分解（Matrix Factorization）
    - 定义用户向量矩阵与item向量矩阵，两者矩阵乘积为共现矩阵。
    - 利用交替最小二乘（ALS）方法求解，并可以设置正则项。
- 逻辑回归（Logistic Regression)
    - 本质上是利用线性回归学习几率。
    - 优点是结构简单，可解释性强；缺点是表达能力不强。
- FM与FFM
    - FM能够学习特征的交叉信息，详见FM论文笔记。
    - FFM引入了Filed的概念，特征被划分为多个Filed，每个特征对与不同的Field设置不同的隐向量。
- GBDT+LR
    - 本质上是利用GBDT来做特征工程，再用LR模型做训练。
    - GBDT输出的是样本落入各决策树叶子节点的编号向量。
- 混合逻辑回滚（MLR）
    - 本质上对于不同的数据利用不同LR函数来拟合。
    - 例如利用softmax函数来对所有的LR函数输出做加权求和，有点像attention机制。
 
## 第三章 - 深度学习在推荐系统中的应用

- 自编码器（AutoRec）
    - 三层神经网络，输入层是评分向量，训练使得输出层和输入层的误差最小（只考虑输入层有值的误差），输出层即是预测的评分向量。
- Deep Cross模型
    - 特征embedding后拼接并进入mlp，相比之前的突破点是特征embedding。
- NeuralCF
    - 矩阵分解其实就是使得user向量和item向量内积为评分值，NeuralCF将内积操作改成mlp。
    - 这里能联想到DSSM的结构，矩阵分解是直接内积，DSSM是先将两个向量经过mlp再内积。
    - NeuralCF和矩阵分解是直接对id进行embedding，而DSSM考虑了其他的特征。
- PNN（product）
    - 特征embedding后进行交叉内积并进入mlp，相当于是在FM上加mlp。
    - 乘积层包括内积和外积操作，为了减小模型负担，该模型提出将所有特征的外积矩阵求和，这个操作会模糊掉特征中的信息，一般不建议使用。
- Wide&deep
    - 主要目的是同时利用先验信息和潜在信息，详见Wide&deep论文笔记。
    - 之后的很多论文都是在此基础上改进wide/deep部分的结构。
- DCN（deep&cross)
    - 利用cross结构代替wide部分，cross结构利用向量外积将特征充分交叉，并在每一层都保留了输入向量，模型的非线性学习能力更强。
    - 我理解这边可以将特征外积与输入向量分开，这样可以更针对性的学习特征交叉信息。
- FM和深度学习的结合
    - FNN，为了加速embedding部分的收敛速度，利用FM做预训练，将FM各特征的隐向量来初始化dnn的特征embedding参数。
    - DeepFM，将FM结构代替wide部分，同时具备了特征低阶交叉和高阶交叉，并利用embedding解决了稀疏特征的参数规模较大问题。
    - NFM，FM二阶交叉部分用的是内积，在NFM中用了向量元素乘法保留了整个向量，并将所有交叉向量求和输出到mlp中。
- attention在推荐模型中的应用
    - AFM，在特征交叉层之后加入attention结构，对下一层的输出进行加权求和，是模型结构上的一次尝试。
    - DIN，针对业务特点在用户历史行为中加入attention结构，详见DIN笔记。
    - DIEN，引入用户历史行为的"序列信息"，详见DIEN笔记。
- 强化学习和推荐系统的结合
    - 介绍的比较简略。
    
## 第四章 - embedding在推荐系统中的应用

- Word2vec
    - 经典的embedding技术，详见word2vec笔记。
- Item2vec - word2vec在推荐领域的推广
    - word中序列信息在item中可以理解为用户的item行为序列，与word不同的点是，item在embedding的时候可以不考虑时序信息。
    - 广义的item2vec，类似与DSSM的双塔模型。
    - item2vec的缺点是只能利用序列型的数据来建立item间的相关性。
- Graph embedding - 引入更多的信息
    - DeepWalk：在物品组成的图上随机游走产生item序列，利用word2vec方法训练。
    - Node2vec：与DeepWalk相比优化来游走方法，通过限制转移概率来权衡BFS（结构性相似）和DFS（节点内容相似）。
    - GES：word2vec模型输入除了itemId还增加了brandId，catId等embedding，可以训练得到多个维度的embedding，最后求平均得到最后的向量，能够一定程度上解决冷启动问题。
    - EGES：考虑到不同的类别embedding对结果贡献不同，将上述求平均替换为attention的结构。
- embedding与推荐系统的结合
    - 利用embedding预训练，作为特征的embedding层。
    - 利用embedding向量的相似度做召回。
- 局部敏感hash
    - 基本思想：在高维数据空间中的两个相邻的数据被映射到低维数据空间中后，将会有很大的概率任然相邻。通过这样一映射，我们可以在低维数据空间来寻找相邻的数据点，减少搜索复杂度，有这样性质的哈希映射称为是局部敏感的。
    - 数据描述：若d(x1,x2)<r1，那么P[h(x1)=h(x2)]≥p1；若d(x1,x2)>r2，那么P[h(x1)=h(x2)]≤p2。即当x1和x2的相似度与hash之后在同一个桶里的概率成正相关。
    - 建立索引：将数据通过一组LSH函数进行降维得到"低维矩阵"，将低维矩阵的特征维度切分为若干组，每一组通过一个常规的哈希函数进行映射（相似度判断），此时如果映射的是同一个桶说明样本在这组特征上是相似的。
    - 在线检索：将数据进过LSH方法得到桶号，低维矩阵特征维度被分为若干组，只要这些组中有n个以上是在同一个桶即认为样本在同一个桶，此时取出同一桶内的数据做判断即可（为了进一步减少计算量可以随机取n个）。
    
## 第五章 - 多角度审视推荐系统

- 特征工程
    -待补充笔记

