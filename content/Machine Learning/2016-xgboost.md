---
title: "2016-XGBoost: A Scalable Tree Boosting System"
layout: page
date: 2020-06-21
---


## 总结

- GBDT在最小化损失函数中，利用CART拟合负梯度，并做boost学习。
- XgBoost则将损失函数二阶泰勒展开，求得解析解，并以解析解的损失函数做为标准，贪心的搜索生成子树。
- 本质上GBDT和XgBoost在增量学习中的思路是一样的，只是一个用了梯度下降法，一个用了牛顿法。由于XgBoost中添加了L2正则，所以和普通的牛顿法有所区别。

## 主要内容

- 推导了损失函数二阶泰勒展开求解析解的过程，并以该解析解来指导子树的生成。
- 为了防止过拟合，新子树合并时需要乘以一个缩减权值（Shrinkage），并在子树分裂时对搜索特征做随机采样（Column Subsampling），全局/按层采样。
- 对于连续值进行分桶，只对各分位点进行搜索，包括全局近似搜索与局部近似搜索（分桶是否在分裂时改变）。
- 对缺失值的处理（特征稀疏），在搜索分裂点时将缺失的数据全都归为左边/右边，选取得分最大的分分裂点。
- 系统设计
    - 树模型学习最耗时的是排序，预先对特征进行排序，后期节点分裂可以复用全局排序结果，文中成为Block结构。
    - 由于Block结构通过索引取梯度，所以会导致非连续的内存访问，影响算法效率。所以本文提出在节点分裂时，将梯度信息存入缓存，提高算法效率。
    - 有关系统设计的之后再具体研究下。